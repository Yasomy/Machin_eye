import cv2
import time
import torch
import numpy as np
from ultralytics import YOLO
from deep_sort_realtime.deepsort_tracker import DeepSort  # pip install deep_sort_realtime
import random


class ObjectDetectionStream:
    def __init__(self, stream_url, input_size=1920):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞ —Å —Ç—Ä–µ–∫–∏–Ω–≥–æ–º.
        :param stream_url: URL –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞
        :param input_size: —Ä–∞–∑–º–µ—Ä –∫–∞–¥—Ä–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
        """
        self.stream_url = stream_url
        self.input_size = input_size
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"üîπ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device.upper()}")
        self.model = self.load_model()
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∫–ª–∞—Å—Å–æ–≤
        self.CLASS_NAMES_DICT = self.model.model.names

    def load_model(self):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å YOLO –∏ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –µ–µ –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ.
        """
        model = YOLO("best_x4.pt").to(self.device)
        # –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å fuse() –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è:
        model.fuse()
        return model

    def predict(self, frame):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –¥–∞–Ω–Ω–æ–º –∫–∞–¥—Ä–µ.
        :param frame: –∫–∞–¥—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Ä–∞–∑–º–µ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å input_size x input_size)
        :return: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ—Ç–µ–∫—Ü–∏–∏
        """
        results = self.model(frame)
        return results

    def get_results(self, results, original_w, original_h):
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ person (0) –∏ transport (2)
        –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –æ–±—Ä–∞—Ç–Ω–æ –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—é.
        
        :param results: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–∏
        :param original_w: –∏—Å—Ö–æ–¥–Ω–∞—è —à–∏—Ä–∏–Ω–∞ –∫–∞–¥—Ä–∞
        :param original_h: –∏—Å—Ö–æ–¥–Ω–∞—è –≤—ã—Å–æ—Ç–∞ –∫–∞–¥—Ä–∞
        :return: (detections, person_count, transport_count)
          detections ‚Äì numpy-–º–∞—Å—Å–∏–≤, –≥–¥–µ –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –∏–º–µ–µ—Ç –≤–∏–¥:
                       [x1, y1, x2, y2, confidence, class_id]
        """
        detections = []
        person_count = 0
        transport_count = 0
        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—é
        scale_x = original_w / self.input_size
        scale_y = original_h / self.input_size

        for box in results[0].boxes:
            cls = int(box.cls[0])
            if cls in [0, 2]:
                bbox = box.xyxy.cpu().numpy()  # —Ñ–æ—Ä–º–∞—Ç [[x1, y1, x2, y2]]
                conf = box.conf.cpu().numpy()[0]
                x1 = bbox[0][0] * scale_x
                y1 = bbox[0][1] * scale_y
                x2 = bbox[0][2] * scale_x
                y2 = bbox[0][3] * scale_y
                detections.append([x1, y1, x2, y2, conf, cls])
                if cls == 0:
                    person_count += 1
                elif cls == 2:
                    transport_count += 1
        detections = np.array(detections) if detections else np.empty((0, 6))
        return detections, person_count, transport_count

    def draw_tracking_boxes(self, frame, tracked_objects):
        """
        –û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ—Ç —Ç—Ä–µ–∫–∏–Ω–≥–æ–≤—ã–µ –±–æ–∫—Å—ã –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ –∫–ª–∞—Å—Å–∞ person —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º (ID).
        :param frame: –∏—Å—Ö–æ–¥–Ω—ã–π –∫–∞–¥—Ä
        :param tracked_objects: —Å–ø–∏—Å–æ–∫ –±–æ–∫—Å–æ–≤ —Å —Ç—Ä–µ–∫–∏–Ω–≥–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π [x1, y1, x2, y2, track_id]
        :return: –∫–∞–¥—Ä —Å –æ—Ç—Ä–∏—Å–æ–≤–∞–Ω–Ω—ã–º–∏ –±–æ–∫c–∞–º–∏
        """
        for obj in tracked_objects:
            x1, y1, x2, y2, track_id = obj
            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 1)
            cv2.putText(frame, f"ID: {int(track_id)}", (int(x1), int(y1)-10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 1)
        return frame

    def draw_transport_boxes(self, frame, detections):
        """
        –û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ—Ç –±–æ–∫—Å—ã –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ –∫–ª–∞—Å—Å–∞ transport.
        :param frame: –∏—Å—Ö–æ–¥–Ω—ã–π –∫–∞–¥—Ä
        :param detections: –º–∞—Å—Å–∏–≤ –¥–µ—Ç–µ–∫—Ü–∏–π [x1, y1, x2, y2, conf, class_id]
        :return: –∫–∞–¥—Ä —Å –æ—Ç—Ä–∏—Å–æ–≤–∞–Ω–Ω—ã–º–∏ –±–æ–∫c–∞–º–∏ –¥–ª—è —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞
        """
        for det in detections:
            x1, y1, x2, y2, conf, cls = det
            if int(cls) == 2:
                # –¢–æ–Ω–∫–∞—è –æ–±–≤–æ–¥–∫–∞: —Ç–æ–ª—â–∏–Ω–∞ 1, —Ç–µ–∫—Å—Ç –Ω–µ –æ—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ–º
                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 1)
        return frame

    def __call__(self):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞.
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ, —Ç—Ä–µ–∫–∏–Ω–≥ –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ –∫–ª–∞—Å—Å–∞ person —á–µ—Ä–µ–∑ DeepSORT,
        –æ—Ç—Ä–∏—Å–æ–≤–∫—É —Ç—Ä–µ–∫–∏–Ω–≥–æ–≤—ã—Ö –±–æ–∫—Å–æ–≤ –∏ –±–æ–∫—Å–æ–≤ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞, –≤—ã–≤–æ–¥ FPS –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç—ã.
        """
        cap = cv2.VideoCapture(self.stream_url)
        if not cap.isOpened():
            print("‚ùå –û—à–∏–±–∫–∞: OpenCV –Ω–µ –º–æ–∂–µ—Ç –æ—Ç–∫—Ä—ã—Ç—å –ø–æ—Ç–æ–∫!")
            return

        original_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        original_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        if original_w == 0 or original_h == 0:
            print("‚ùå –û—à–∏–±–∫–∞: –Ω–µ–≤–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤–∏–¥–µ–æ!")
            return

        num = 1  # –Ω–æ–º–µ—Ä –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤
        prev_time = time.time()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º DeepSORT —Ç—Ä–µ–∫–µ—Ä –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ –∫–ª–∞—Å—Å–∞ person
        # –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–∂–Ω–æ –ø–æ–¥–±–∏—Ä–∞—Ç—å, –∑–¥–µ—Å—å –ø—Ä–∏–≤–µ–¥—ë–Ω –ø—Ä–∏–º–µ—Ä —Å n_init=5 –∏ max_age=1000
        tracker = DeepSort(max_age=1000, n_init=15, max_cosine_distance=0.3, embedder="mobilenet", embedder_gpu=True)

        while True:
            ret, frame = cap.read()
            if not ret or frame is None:
                print("‚ùå –û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä.")
                break

            # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è –ø–æ–¥–∞—á–∏ –≤ –º–æ–¥–µ–ª—å
            resized_frame = cv2.resize(frame, (self.input_size, self.input_size))
            results = self.predict(resized_frame)
            # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ person –∏ transport
            detections, person_count, transport_count = self.get_results(results, original_w, original_h)

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π –¥–ª—è —Ç—Ä–µ–∫–∏–Ω–≥–∞ (—Ç–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç—ã –∫–ª–∞—Å—Å–∞ person)
            # –î–ª—è DeepSORT —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç [xmin, ymin, width, height, confidence]
            person_detections_deep = []
            for det in detections:
                if int(det[5]) == 0:
                    x1, y1, x2, y2, conf = det[0], det[1], det[2], det[3], det[4]
                    width = x2 - x1
                    height = y2 - y1
                    person_detections_deep.append([[x1, y1, width, height], conf])

            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç—Ä–µ–∫–∏–Ω–≥ —á–µ—Ä–µ–∑ DeepSORT. –ï—Å–ª–∏ –Ω–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏–π, –ø–µ—Ä–µ–¥–∞—ë–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.
            tracks = tracker.update_tracks(person_detections_deep, frame=frame)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–æ–∫—Å—ã –∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Ç—Ä–µ–∫–æ–≤
            tracked_objects = []
            for track in tracks:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω –ª–∏ —Ç—Ä–µ–∫
                if not track.is_confirmed():
                    continue
                bbox = track.to_ltrb()  # –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç [x1, y1, x2, y2]
                track_id = track.track_id
                tracked_objects.append([bbox[0], bbox[1], bbox[2], bbox[3], track_id])

            # –û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ–º —Ç—Ä–µ–∫–∏–Ω–≥–æ–≤—ã–µ –±–æ–∫—Å—ã –¥–ª—è persons
            frame = self.draw_tracking_boxes(frame, tracked_objects)
            # –û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ–º –±–æ–∫—Å—ã –¥–ª—è —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞
            frame = self.draw_transport_boxes(frame, detections)

            # –†–∞—Å—á–µ—Ç FPS
            curr_time = time.time()
            fps = 1 / (curr_time - prev_time) if (curr_time - prev_time) > 0 else 0
            prev_time = curr_time

            cv2.putText(frame, f"FPS: {int(fps)}", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)
            cv2.putText(frame, f"People: {person_count}", (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 1)
            cv2.putText(frame, f"Transport: {transport_count}", (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 255), 1)

            cv2.imshow("YOLO Stream with DeepSORT (GPU)", frame)

            key = cv2.waitKey(1)
            if key == ord("q"):
                break
            elif key == ord("s"):
                cv2.imwrite(f'images/img{num}.png', frame)
                print("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ!")
                num += 1

        cap.release()
        cv2.destroyAllWindows()


if __name__ == '__main__':
    stream_url = (
        "http://46.191.199.12/1660720512DSH176/index.fmp4.m3u8?token=7acaece6c9fc4550a83d2ee1e4316e4e"
    )
    detector = ObjectDetectionStream(stream_url)
    detector()
